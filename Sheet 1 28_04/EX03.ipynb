{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae27db5",
   "metadata": {},
   "source": [
    "# Machine Learning Essentials SS25 - Exercise Sheet 1\n",
    "\n",
    "## Instructions\n",
    "- `TODO`'s indicate where you need to complete the implementations.\n",
    "- You may use external resources, but <b>write your own solutions</b>.\n",
    "- Provide concise, but comprehensible comments to explain what your code does.\n",
    "- Code that's unnecessarily extensive and/or not well commented will not be scored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5592766",
   "metadata": {},
   "source": [
    "TODO: Interpret the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8fbb30",
   "metadata": {},
   "source": [
    "## Exercise 3: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb1c0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles, make_blobs\n",
    "from cvxopt import matrix, solvers # Install cvxopt via \"pip install cvxopt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 1. Complete SVM implementation\n",
    "# ========================\n",
    "\n",
    "class DualSVM:\n",
    "    def __init__(self, C=1.0, kernel=\"linear\", gamma=1.0):\n",
    "        self.C = C # Regularization constant\n",
    "        self.kernel = kernel # Kernel type: \"linear\" or \"rbf\"\n",
    "        self.gamma = gamma # Kernel parameter (\"bandwith\")\n",
    "        self.alpha = None # Lagrange multipliers\n",
    "        self.sv_X = None # Support vectors\n",
    "        self.sv_y = None # Support vector labels\n",
    "        self.w = None # Weights\n",
    "        self.b = None # Bias\n",
    "\n",
    "    def linear_kernel(self, X1, X2):\n",
    "        #TODO: Implement linear kernel\n",
    "        return \n",
    "\n",
    "    def rbf_kernel(self, X1, X2):\n",
    "        #TODO: Implement RBF kernel \n",
    "        return \n",
    "\n",
    "    def compute_kernel(self, X1, X2):\n",
    "        if self.kernel == \"linear\":\n",
    "            return self.linear_kernel(X1, X2)\n",
    "        elif self.kernel == \"rbf\":\n",
    "            return self.rbf_kernel(X1, X2)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown kernel type.\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Compute kernel matrix K: K[i,j] = K(x_i, x_j)\n",
    "        K = self.compute_kernel(X, X)\n",
    "\n",
    "        \"\"\"\n",
    "        The dual objective is:\n",
    "            max  sum_i alpha_i - 1/2 sum_i sum_j alpha_i alpha_j y_i y_j K(x_i, x_j)\n",
    "        subject to:\n",
    "            sum_i alpha_i y_i = 0  and  0 <= alpha_i <= C for all i.\n",
    "        In QP formulation:\n",
    "            P = (y_i y_j K(x_i,x_j))_{i,j},   q = -1 (vector),\n",
    "            A = y^T, b = 0, and G, h implement 0 <= alpha_i <= C.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Use the matrix function of cvxopt to define the QP parameters\n",
    "        P = matrix()\n",
    "        q = matrix()\n",
    "        A = matrix(y, (1, n_samples),\"d\") # Use \"d\" flag to make sure that the matrix is in double precision format (labels are integers)\n",
    "        b = matrix()\n",
    "        \n",
    "        \n",
    "        # TODO: Implement inequality constraints by defining G and h\n",
    "        G = matrix()\n",
    "        h = matrix()\n",
    "\n",
    "        # Solve the QP problem using cvxopt\n",
    "        solvers.options[\"show_progress\"] = False\n",
    "        solution = solvers.qp(P, q, G, h, A, b)\n",
    "        alphas = np.ravel(solution[\"x\"]) # Get optimal alphas\n",
    "\n",
    "        # Get support vectors (i.e. data points with non-zero lagrange multipliers, that are on the margin)\n",
    "        sv = alphas > 1e-5 # alpha > 1e-5 to account for numerical errors\n",
    "        self.alpha = alphas[sv]\n",
    "        self.sv_X = X[sv]\n",
    "        self.sv_y = y[sv]\n",
    "\n",
    "        # The bias corresponds to the average error over all support vectors:\n",
    "        # Why does the bias corresponds to the average error over all support vectors?\n",
    "        # The answer is that the bias is the average of the differences between the true labels and the predicted labels\n",
    "        # for the support vectors. The predicted labels are computed by the decision function f(x) = sum(alpha_i y_i K(x,x_i)) + b.\n",
    "        # The difference between the true labels and the predicted labels is the error for each support vector.\n",
    "        # The bias is the average of these errors.\n",
    "        self.b = np.mean(self.sv_y - np.sum(self.alpha * self.sv_y * K[sv][:, sv], axis=1))\n",
    "\n",
    "    def predict(self, X):\n",
    "        #TODO: Implement the decision function and return the corresponding predicted labels\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53972b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 2. Apply linear SVM on blobs\n",
    "# ========================\n",
    "\n",
    "# TODO: Generate blobs dataset\n",
    "X_linear, y_linear = \n",
    "\n",
    "# Convert labels from {0,1} to {-1,1}\n",
    "y_linear = 2 * (y_linear - 0.5) \n",
    "\n",
    "#TODO: Train SVM with linear kernel\n",
    "\n",
    "#TODO: Plot decision boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b92e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 3. Apply linear SVM on circles\n",
    "# ===========================\n",
    "\n",
    "#TODO: Generate circles dataset\n",
    "X_circles, y_circles = \n",
    "y_circles = 2 * (y_circles - 0.5)  # Convert labels from {0,1} to {-1,1}\n",
    "\n",
    "#TODO: Train SVM with linear kernel\n",
    "\n",
    "#TODO: Plot decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 4. Apply feature transformation \n",
    "# ===========================\n",
    "\n",
    "def transform_features(X):\n",
    "    # TODO: compute feature transformation: f(x) = [x1, x2, x1^2 + x2^2]\n",
    "    return \n",
    "\n",
    "\n",
    "#TODO: Train SVM with linear kernel on transformed features\n",
    "\n",
    "def plot_decision_boundary_transformed(X, y, model, title=\"SVM Decision Boundary (Transformed)\"):\n",
    "    # TODO: Implement plotting function for decision boundary in the transformed feature space\n",
    "    # Hint: You could do this by creating a 2D meshgrid which you transform using the feature mapping.\n",
    "    # Then, after evaluating the model on it, you can plot the result as a contour plot (plt.contourf).\n",
    "    ...\n",
    "    plt.show()\n",
    "\n",
    "#TODO: Plot decision boundary in the transformed feature space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ff899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 5. SVM with RBF Kernel on Circular Data\n",
    "# ===========================\n",
    "\n",
    "#TODO: Train SVM with RBF kernel on circular data\n",
    "\n",
    "#TODO: Plot decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de0adcd",
   "metadata": {},
   "source": [
    "### 6.\n",
    "\n",
    "TODO: Compare the decision boundaries from Tasks 3, 4, and 5. How does feature transformation differ from using an RBF kernel? When would one approach be preferable to the other?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e19ad",
   "metadata": {},
   "source": [
    "### 7. \n",
    "\n",
    "TODO: Besides the dual formulation, SVMs also have an equivalent primal formulation. The key factor in choosing which one to use as the optimization criterion is the dimensionality of the features. Explain why."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
